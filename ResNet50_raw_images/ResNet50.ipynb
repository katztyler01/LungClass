{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import Model\n",
    "from keras.layers import GlobalAveragePooling2D, Dense, LeakyReLU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "from tensorflow_addons.metrics import F1Score\n",
    "from tensorflow.keras.applications.resnet import preprocess_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../Data_Entry_2017.csv\") \n",
    "filenames = [] \n",
    "directories = []\n",
    "# parse each of the filenames used in analysis, and filter dataframe accordingly\n",
    "with open(\"../selected_png_list.txt\", \"r\") as fr: # read all 9600 selected filenames\n",
    "    for line in fr.readlines():\n",
    "        start = line.find(\"images_0\")\n",
    "        directory = \"images/\" + line[start:]\n",
    "        filename_reverse = line[::-1]\n",
    "        filename_reverse = filename_reverse[:filename_reverse.find(\"/\")]\n",
    "        filename = filename_reverse[::-1]\n",
    "        filenames.append(filename[:-1])\n",
    "        directories.append(directory)\n",
    "data = data[data[\"Image Index\"].isin(filenames)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treat multiple classifications from individual radiologists equally\n",
    "# assigns new row for each individual classification present (classifications separated by \"|\" in dataset)\n",
    "for index,row in data.iterrows():\n",
    "    label = row[\"Finding Labels\"]\n",
    "    if label.find(\"|\") != -1:\n",
    "        labels = label.split(\"|\") # split by \"|\" to get all labels\n",
    "        newrow = row \n",
    "        inserted_data = []\n",
    "        for label in labels:\n",
    "            newrow[\"Finding Labels\"] = label # make new row according to each label\n",
    "            inserted_data.append(newrow)\n",
    "        inserted_data = pd.DataFrame(inserted_data)\n",
    "        print(inserted_data)\n",
    "        data = pd.concat([data, inserted_data],ignore_index=True) # concatenate new rows and original data\n",
    "        \n",
    "# step to remove each of the rows that contain \"|\" because they represent multiple classifications from different radiologists\n",
    "# each image gets placed into separate rows in the dataframe according to each classification, so each classification is used in analysis\n",
    "for index, row in data.iterrows(): \n",
    "    if row[\"Finding Labels\"].find(\"|\") != -1:\n",
    "        data.drop(index, inplace=True)\n",
    "data.to_csv(\"full_data.csv\") # store adjusted new data into full_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications = np.unique(data.loc[:,\"Finding Labels\"])\n",
    "print(len(classifications))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting train and test set (80/20), same split used for other methods\n",
    "train_full_set = []\n",
    "test_full_set = []\n",
    "data = pd.read_csv(\"full_data.csv\") \n",
    "for classification in classifications: # iterate over each disease classification\n",
    "    class_filter = data[data[\"Finding Labels\"] == classification]\n",
    "    print(classification, len(class_filter))\n",
    "    indices_len = len(class_filter)\n",
    "    if indices_len > 400: # attempt to adjust for class imbalance by limiting higher # class to 400\n",
    "        indices_len = 400\n",
    "    train_set_size = math.floor(0.8 * indices_len)\n",
    "    \n",
    "    # get random indices to select for train and test set\n",
    "    permutation = np.random.permutation(np.arange(0,indices_len))\n",
    "    permutation = [int(x) for x in permutation]\n",
    "    \n",
    "    train_filenames = class_filter.iloc[permutation[:train_set_size], 1] # filter dataframe by index to find randomized indices\n",
    "    test_filenames = class_filter.iloc[permutation[train_set_size:], 1] # same for test set, but for other 20%\n",
    "\n",
    "    for filename in train_filenames:\n",
    "        train_full_set.append(filename) # append filename to train set\n",
    "    for filename in test_filenames:\n",
    "        test_full_set.append(filename) # append filenmae to test set\n",
    "\n",
    "train_data = data[data[\"Image Index\"].isin(train_full_set)] # filter dataframe according to filenames found in train set\n",
    "test_data = data[data[\"Image Index\"].isin(test_full_set)]  # do same for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications_observed = classifications[classifications != \"No Finding\"] \n",
    "model_predictions = []\n",
    "histories = []\n",
    "for classification in classifications_observed: # iterate over all disease classifications\n",
    "        # assemble ResNet50 model\n",
    "        base_model = ResNet50(include_top=False, \n",
    "                                    classes=1,\n",
    "                                    input_shape=(1024,1024,3))\n",
    "        x = base_model.output\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = LeakyReLU(alpha = 0.4)(x) \n",
    "        predictions = Dense(1, activation='sigmoid')(x)\n",
    "        \n",
    "        model = Model(inputs=base_model.input, outputs=predictions)\n",
    "        \n",
    "        # compile ResNet50 model\n",
    "        model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy', Precision(), Recall(), F1Score(num_classes=1, average = \"weighted\"), AUC(curve = \"ROC\")])\n",
    "\n",
    "        print(f\"Training ResNet50 model on {classification}\")\n",
    "        possible_classes = [classification, \"No Finding\"]\n",
    "        \n",
    "        # prepare train set data\n",
    "        train_data_subset = train_data[train_data[\"Finding Labels\"].isin(possible_classes)]\n",
    "        train_datagen = ImageDataGenerator(rescale = 1./255, preprocessing_function=preprocess_input)\n",
    "        train_generator = train_datagen.flow_from_dataframe(\n",
    "                train_data_subset,\n",
    "                x_col = \"Image Index\",\n",
    "                y_col = \"Finding Labels\",\n",
    "                directory = \"images\",\n",
    "                target_size=(224,224),\n",
    "                batch_size=16,\n",
    "                class_mode='binary',\n",
    "                shuffle=True)\n",
    "\n",
    "        # fit model on train_set \n",
    "        history = model.fit(\n",
    "                train_generator,\n",
    "                steps_per_epoch=len(train_generator),\n",
    "                epochs=10,\n",
    "                verbose=1)\n",
    "        \n",
    "        \n",
    "        print(f\"Testing ResNet50 model on {classification}\")\n",
    "         # test data preparation\n",
    "        test_data_subset = test_data[test_data[\"Finding Labels\"].isin(possible_classes)]\n",
    "        test_datagen = ImageDataGenerator(rescale=1./255,preprocessing_function=preprocess_input)\n",
    "        test_generator = test_datagen.flow_from_dataframe(\n",
    "                test_data_subset,\n",
    "                x_col = \"Image Index\",\n",
    "                y_col = \"Finding Labels\",\n",
    "                directory = \"images\",\n",
    "                target_size=(224,224),\n",
    "                batch_size=16,\n",
    "                class_mode='binary',\n",
    "                shuffle=True)\n",
    "        \n",
    "        # append train_set model to histories\n",
    "        histories.append(history)\n",
    "        model_predictions.append(model.predict(test_generator)) \n",
    "        model.evaluate(test_generator) # run model on test set\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
